# MERIDIAN DEMO SCRIPT — 5 MINUTES

**Presenter Preparation:**
- Have deck.html open in browser (full screen)
- Have Meridian app open in another tab/window
- Ensure server is running with engine available
- Run demo reset before starting: `Invoke-RestMethod -Uri 'http://localhost:8000/api/demo/reset' -Method POST`
- Have the copilot tab and dashboard tab ready to switch between

---

## [0:00 - 0:30] SLIDES 1-2: Context Setting (30 seconds)

**SLIDE 1** — on screen.

> "Good morning. Meridian is a self-learning AI copilot for support agents. Every answer is evidence-grounded — traced back to a specific ticket, conversation, or script. And the system gets smarter from every interaction."

**ACTION:** Click to advance to SLIDE 2.

> "The dataset has over 3,200 KB articles, but 95% have no metadata. 714 scripts exist but agents struggle to find the right one. And when agents solve new problems, that knowledge disappears into closed tickets. Meridian fixes this."

**ACTION:** Click to advance to SLIDE 3 (Architecture).

> "The architecture has three surfaces: the Copilot gives agents evidence-grounded answers. The Learning Engine detects gaps and generates new knowledge. The Dashboard governs quality and compliance."

---

## [0:30 - 1:30] COPILOT DEMO (60 seconds)

**ACTION:** Switch to the Meridian app — Copilot tab.

**ACTION:** Type in the query box:
```
advance property date backend script fails
```

**ACTION:** Press Search/Submit.

**WAIT:** Results populate (should take ~1 second).

> "The engine classified this as a SCRIPT question with 82% confidence. It pulled the right backend fix script — SCRIPT-0293 — with the required input parameters highlighted. Notice the provenance badge on the KB article result."

**ACTION:** Click the provenance badge (◉ Evidence Chain) on one of the KB-SYN articles in the secondary results.

**WAIT:** Provenance modal opens showing the evidence chain.

> "This article was automatically generated from ticket CS-38908386, which captured a conversation with agent Alex, where they used this exact script to resolve the issue. Every recommendation traces to its source. Nothing is a black box."

**ACTION:** Close provenance modal (click X or backdrop).

---

## [1:30 - 2:15] DASHBOARD + EVAL (45 seconds)

**ACTION:** Switch to Dashboard tab.

> "The dashboard shows knowledge health — 161 articles have been auto-generated by the learning system from resolved tickets. Our eval harness ran 1,000 ground-truth questions against the corpus."

**ACTION:** Point to (or hover over) the retrieval accuracy metrics.

> "Hit@5 retrieval accuracy is [Y]%. The classification accuracy is 71%."

**ACTION:** Point to the Self-Learning Proof card (before/after comparison).

> "This is the headline: before the learning loop, hit@5 was [A]%. After the system generated 161 articles from resolved tickets, it jumped to [B]%. That's [C] percentage points of improvement."

**PAUSE.**

> "But that's historical. Let me show you what happens when the system encounters something completely new — in real time."

---

## [2:15 - 3:15] LIVE INJECTION — THE WOW MOMENT (60 seconds)

**ACTION:** Stay on Dashboard (or switch to Demo panel if built).

> "We just received 6 new tickets about a problem Meridian has never seen: Report Export Failure. Customers getting blank PDFs, wrong data in exports, timeouts on large reports. Nothing about this issue exists in our knowledge base."

**ACTION:** Click "Inject Tickets" button (or trigger via API/demo panel).

Alternate if using API directly:
```powershell
Invoke-RestMethod -Uri 'http://localhost:8000/api/demo/inject' -Method POST
Invoke-RestMethod -Uri 'http://localhost:8000/api/demo/detect-gaps' -Method POST
Invoke-RestMethod -Uri 'http://localhost:8000/api/demo/detect-emerging' -Method POST
```

**WAIT:** Tickets inject, gap detection runs, emerging issue appears.

> "The gap detector just ran on all 6 tickets. Every single one flagged as a knowledge gap — zero KB coverage, similarity scores below 0.15. The system has no idea how to answer questions about report exports."

**ACTION:** Point to emerging issues section (should show "Report Export Failure — 6 tickets, 0 KB coverage").

> "The emerging issue detector clustered these automatically: 'Report Export Failure — 6 unmatched tickets, no KB coverage.' The system identified a pattern across multiple customers hitting the same unknown problem."

---

## [3:15 - 3:45] DRAFT + APPROVE (30 seconds)

**ACTION:** Trigger KB draft generation (button or API).

Alternate if using API:
```powershell
Invoke-RestMethod -Uri 'http://localhost:8000/api/demo/generate-draft' -Method POST
```

**WAIT:** Draft appears in the approval queue (1-2 seconds).

**ACTION:** Point to the generated draft in the Learning Pipeline section.

> "Meridian drafted a KB article from the ticket data. It synthesized the common pattern — report cache issues, encoding problems, timeout configuration — into a structured troubleshooting guide."

**ACTION:** Click "Approve" button on the draft.

Alternate if using API:
```powershell
Invoke-RestMethod -Uri 'http://localhost:8000/api/demo/approve' -Method POST
```

**WAIT:** Approval processes (~1 second).

> "Approved. The article is now in the retrieval index — it's live."

---

## [3:45 - 4:30] RETRIEVAL VERIFICATION — THE PAYOFF (45 seconds)

**ACTION:** Switch to Copilot tab.

**ACTION:** Clear the previous query and type:
```
Customer getting blank PDFs when exporting Rent Roll report
```

**ACTION:** Press Search/Submit.

**WAIT:** Results populate.

> "There it is."

**ACTION:** Point to the newly created KB article in the results (should be KB-DRAFT-XXX or similar).

> "The copilot is now returning the article that was created 60 seconds ago. The provenance shows it was generated from the demo tickets we just injected. The system went from 'I don't know about this' to 'Here's an evidence-grounded answer with full traceability' — in under a minute."

**PAUSE for emphasis.**

> "That's the self-learning loop, live. Not historical replay — real-time adaptation to a problem that didn't exist in the knowledge base."

---

## [4:30 - 5:00] SLIDES 5-6: Closing (30 seconds)

**ACTION:** Switch back to deck.html.

**ACTION:** Advance to SLIDE 5 (The Numbers).

> "The numbers: retrieval accuracy at hit@5 is [Y]%, the before/after learning loop showed a [C] percentage point improvement, and we just demonstrated live learning in under 60 seconds."

**ACTION:** Advance to SLIDE 6 (Enterprise Vision).

> "In production: live ticket feeds from Salesforce or Zendesk, org-wide emerging issue detection, automated QA scoring against compliance rubrics, and every answer with provenance. Every answer comes with a citation. Nothing is a black box."

**PAUSE.**

> "Thank you."

---

## BACKUP PLAN

If the live demo fails (API errors, timing issues, etc.):

1. **Fallback to recorded demo**: Have screenshots or a screen recording ready
2. **Explain the flow**: Walk through what would happen verbally while showing the UI
3. **Show the code**: Open `demo_pipeline.py` and explain the 6-step process
4. **Emphasize the architecture**: The system is built, it just needs the engine to boot

## TIMING BREAKDOWN

| Section | Duration | Cumulative |
|---------|----------|------------|
| Slides 1-2: Context | 30s | 0:30 |
| Copilot Demo | 60s | 1:30 |
| Dashboard + Eval | 45s | 2:15 |
| Live Injection | 60s | 3:15 |
| Draft + Approve | 30s | 3:45 |
| Retrieval Verification | 45s | 4:30 |
| Slides 5-6: Closing | 30s | 5:00 |

**Total: 5:00 minutes**

## KEY TALKING POINTS

- **Evidence-grounded**: Every answer traces to a source
- **Self-learning**: System gets smarter from every resolved ticket
- **Real-time adaptation**: Not historical replay — learns from novel problems live
- **Enterprise-ready**: Provenance, compliance, QA scoring built in
- **Not a black box**: Full traceability from query to source

## COMMON JUDGE QUESTIONS

**Q: How does this differ from RAG?**
A: RAG retrieves existing knowledge. Meridian also generates new knowledge from tickets, indexes it automatically, and traces everything to source evidence.

**Q: What if the generated KB article is wrong?**
A: That's why we have the approval workflow. Senior agents review drafts before they go live. Plus, full provenance means you can always trace back to the source ticket and validate.

**Q: How does this scale?**
A: The gap detector runs on resolved tickets (async batch job), KB generation happens in the background, and retrieval is O(log n) with the TF-IDF index. In production, this would be event-driven with webhooks.

**Q: What about compliance?**
A: The QA scorer evaluates every interaction against a production rubric with red flags for PCI violations, data integrity issues, and unprofessional behavior. Autozero scoring ensures compliance failures are immediately flagged.

**Q: How long did this take to build?**
A: [Answer honestly about timeline and team division of labor]
